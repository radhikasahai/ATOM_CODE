{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyforest\n",
    "data_path = '/Users/jayceepang/msse/ATOM_CODE/original_datasets/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (959015697.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[123], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    assay_pct_dfs = [file for file in os.listdir(datapath) if file.startswith('NEK')]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def check_labels(datapath): \n",
    "  \"\"\"a function to check that the labels are indeed what we want (a % binding or % inhibition activiy is \n",
    "  >= 50% for that molecule to be active against that NEK)\"\"\"\n",
    "   assay_pct_dfs = [file for file in os.listdir(datapath) if file.startswith('NEK')]\n",
    "   for file in assay_pct_dfs:\n",
    "    full_file = os.path.join(datapath, file)\n",
    "    df = pd.read_csv(full_file)\n",
    "    \n",
    "    pct_col = [col for col in df.columns if col.startswith('pct_')] \n",
    "    len(pct_col) == 1 \n",
    "    pct_col = pct_col[0]\n",
    "    invalid_labels = df[df['active']!= (df[pct_col]>=50).astype(int)]\n",
    "    if invalid_labels.empty: \n",
    "      # print(f'correct label assignment based on {pct_col} column')\n",
    "      pass\n",
    "    else: \n",
    "      print(f'Error on label assignement.')\n",
    "      print(invalid_labels)\n",
    "    # check for duplicates here \n",
    "    # duplicates = df[df.duplicated()]\n",
    "    print(f'{df.shape}')\n",
    "    # duplicates = df[df.duplicated(subset=['base_rdkit_smiles'])]\n",
    "    duplicates = df.duplicated()\n",
    "    # duplicates = df[df.duplicated(subset=['base_rdkit_smiles'])]\n",
    "    if duplicates.any():\n",
    "        print('Duplicate rows found:')\n",
    "        print(duplicates)\n",
    "        df_cleaned = df.drop_duplicates()\n",
    "        print(f'df size original: {df.shape}')\n",
    "        print(f'remove duplicates. {df_cleaned.shape}')\n",
    "check_labels(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must specify filename because the 50pct binding and inhibition files differ\n",
    "# to-do: discuss how to do this --> this will create a file that is just this but with column specifying split\n",
    "#       do we want to already integrate a split_uuid like AMPL? \n",
    "#       additionally, we still have to use the splits to create the \"scaled df\" \n",
    "def remove_duplicates(datapath, filename):\n",
    "    df = pd.read_csv(f'{datapath}{filename}')\n",
    "    duplicates = df.duplicated(subset=['base_rdkit_smiles'])\n",
    "    if duplicates.any():\n",
    "        print('Duplicate rows found:')\n",
    "        print(df[duplicates])\n",
    "        df_cleaned = df.drop_duplicates(subset=['base_rdkit_smiles'])\n",
    "        print(f'df size original: {df.shape}')\n",
    "        print(f'Removed duplicates. New df size: {df_cleaned.shape}')\n",
    "    else: \n",
    "        df_cleaned = df\n",
    "    return df_cleaned\n",
    "    \n",
    "def split_data(datapath, filename, train_ratio=.8, test_ratio=0.2): \n",
    "    \"\"\"filename: NEK#_1_uM_min_50_pct_(binding or inhibition).csv\"\"\"\n",
    "    df = remove_duplicates(datapath, filename)\n",
    "\n",
    "    # is this extra/too much and should we just assume what labels coincide with majority and minority? \n",
    "    # determine majority/minority class \n",
    "    #       AMPL takes in 'response column' \n",
    "    class_labels = df['active'].value_counts() \n",
    "    print(class_labels)\n",
    "    if len(class_labels)>1: \n",
    "        majority_class_label =class_labels.idxmax() \n",
    "        majority_num = class_labels.max() \n",
    "        minority_class_label = class_labels.idxmin()\n",
    "        minority_count = class_labels.min()\n",
    "    df_majority = df[df['active']==majority_class_label]\n",
    "    df_minority=df[df['active']==minority_class_label]\n",
    "    # copy to avoid warnings \n",
    "    df_majority = df_majority.copy()\n",
    "    df_minority = df_minority.copy()\n",
    "    n = round(1/test_ratio) # how else can we do this? i think we should keep kfold splits \n",
    "            # 1/.2 = 5 splits \n",
    "    kf = KFold(n_splits=n,shuffle=True, random_state=42)\n",
    "    # majority \n",
    "    for i, (_, v_ind) in enumerate(kf.split(df_majority)):\n",
    "        df_majority.loc[df_majority.index[v_ind], 'fold'] = f\"fold{i+1}\"\n",
    "    # minority \n",
    "    for i, (_, v_ind) in enumerate(kf.split(df_minority)):\n",
    "        df_minority.loc[df_minority.index[v_ind], 'fold'] = f\"fold{i+1}\"\n",
    "    all_fold_df = pd.concat([df_majority,df_minority])\n",
    "    \n",
    "    print(all_fold_df.shape)\n",
    "    print(all_fold_df.active.value_counts())\n",
    "    ## actually, this might be perfect jsut to save as \"split df\" because then \n",
    "    # in the next step of creating a dataset, you can choose what fold you want to be the test set \n",
    "    return all_fold_df\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check for duplicates and split and save splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assay_pct_dfs = [file for file in os.listdir(data_path) if file.startswith('NEK')]\n",
    "\n",
    "def get_bind_inhib(df_name):\n",
    "    match = re.search(r'_([^_]+)\\.csv$',df_name)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "split_dir = '/Users/jayceepang/msse/ATOM_CODE/datasets/'\n",
    "\n",
    "for i in range(len(assay_pct_dfs)): \n",
    "    bind_inhib = get_bind_inhib(assay_pct_dfs[i])\n",
    "    nek_num = str(assay_pct_dfs[i][3])\n",
    "    print(assay_pct_dfs[i])\n",
    "    NEK_name = f'NEK{nek_num}_{bind_inhib}'\n",
    "    print(NEK_name)\n",
    "    cleaned_split_df = split_data(data_path, assay_pct_dfs[i])\n",
    "\n",
    "    # DO WE DO THIS HERE???  \n",
    "    cleaned_split_df['NEK'] =NEK_name \n",
    "    cleaned_split_df.to_csv(f'{split_dir}{NEK_name}_split.csv',index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurization\n",
    "should we do this per dataset? (intend to run this in a for loop?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(train, test): \n",
    "    # file is the split file (folds)\n",
    "    # add code to do standard scaler \n",
    "    # code to scale then concat \n",
    "    scaled_df = pd.concat([train,test])\n",
    "    return scaled_df\n",
    "\n",
    "def featurize(datapath, filename, feat_type, test_fold = 1, n_bits=None): \n",
    "    # if feat_type == 'MOE': \n",
    "        # direct to descriptors files and matchup our split df with the features \n",
    "    # if feat_type == 'MFP' or 'ECFP': \n",
    "        # create MFP feats? \n",
    "        # or should we use ECFP? \n",
    "        # specify bits (is not None)  \n",
    "    # 1. split data based on fold \n",
    "    df = pd.read_csv(datapath+filename) # this is the split.csv \n",
    "    test = df[df['fold']==test_fold]\n",
    "    train=df[df['fold']!=test_fold] \n",
    "    #### HOW DO WE ACCOUNT FOR VALIDATION? ### \n",
    "    # 2. standard scaler \n",
    "    scaled_df = scale_dataset(train,test)\n",
    "    # 3. save scaled_df NO MATTER WHAT.\n",
    "    # ## MAYBE WE SHOULD SAVE IT IN THE SCALE DF FUNCTION?  ## \n",
    "    # 4. perform featurization \n",
    "    # 5. save datasets \n",
    "    # 6. possibly, call function to prep for training (save trainX.csv, train_y.csv, testX.csv, test_y.csv)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # I'm checking the scaled_descriptors files because I knew there were duplicates in these files \n",
    "# desc_dir = '/Users/jayceepang/msse/ATOM_CODE/original_datasets/scaled_descriptors/'\n",
    "# desc_dfs = [file for file in os.listdir(desc_dir) if file.startswith('NEK')]\n",
    "\n",
    "# print(desc_dfs)\n",
    "# for i in range(len(desc_dfs)): \n",
    "#     split_data(desc_dir, desc_dfs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2044"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1904+140=2044"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ATOM_CODE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
